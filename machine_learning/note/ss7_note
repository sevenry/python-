关于adaboost算法的运行，我是跑的《机器学习实战》的源代码，数学解释部分参考的《统计学习方法》一书。
在这里想整理一下，如何避开数学证明的具体过程，解释一下adaboost算法的数学逻辑。

首先是loadSimpData() 函数，产生一系列数据，每一组数据都有几项特征值，在源代码中为两项，还有每组数据的分类结果，记录为1/-1。将特征值和分类结果分别赋给datMat,classLabels。与该函数作用类似的为loadDataSet()函数，通过打开文件，并作一定处理，返回相似结果。

stumpClassify(dataMatrix,dimen,threshVal,threshIneq)函数的作用也很简单，第一项是一系列数据的特征值，dimen是表示此处选择的是第几项特征值作为判断标准，在该章中显然是0/1两项；threshVal是将数据分成两类的临界线，threshIneq是根据临界线分成两组，那么是小于临界线的一组为1，另一组为-1，还是反过来。返回的结果是同该系列数据基于此判据的分类结果，应当是1*数据组数大小。

buildStump(dataArr,classLabels,D)函数的第一项为特征值，第二项为实际分类结果，第三项D为权重。比如说总共有5组数据，那么D有五项，初始的时候每一项值为0.2，保持总和为1且数值相等。函数内部在数据的特征项中（本算例是两项），每一特征项从最低值到最大值，逐步（step可自由设定）增加的所有数字中循环，以求得选择最合适的判据：即确定第一组/第二组数值为特征项，以数字多少为临界线，临界线哪一侧为1类，可以使得判据分类结果与实际结果最相同。
下面说一下误差的计算。生成1*数据组数大小的数值均为1的数列，如果对应索引的判据与实际分类一致则为0。得到的该数列依次乘以对应的D值，并求和。可以看到，如果分类正确，则D失效，分类错误的情况下，才会作为1乘以D对最终误差产生影响。
当当前判据结果产生误差小于最小误差，则更新最小误差为当前判据误差，并保存当前判据。
可以看到的是该函数返回的是一个判据，即简单分类器。

adaBoostTrainDS(dataArr,classLabels,numIt=40)函数接收的参数也是特征值和分类结果，numIt是最多允许的简单分类器个数。这里也是adaboost的核心所在。
首先初始D值，调用buildStump()函数，产生一个分类器。可以想到，当我们的数据很多的时候，我们很难凭借一个数值就将所用数据划分成功。第一个分类器就意味着这一个数值，这是在如果只允许一个分类器时，所能得到的最少分类出错的数据的结果。那么显然，会有一部分数据分类错误，比如说目前十个数据，我以5为界限，小于5的为1，反之为-1。根据这种方法，1,2,6号数据分类错误，我想在这个基础上，继续叠加一个新的分类器，重点修正这三个数据的划分情况。
换句白话说，之前分类正确的七个我可以不那么在意它们在第二轮分类的时候的正误了，我主要关注错的这三个。那么在调用buildStump()函数计算误差时，我们需要放大1,2,6号的D值，加重它们的权重，以尽可能让它们对为目标。而正确的7个，需要降低它们的D值。因为越大的D值会使得在buildStump()函数计算误差时影响程度更高。
为了合理改变D值，函数引入ln计算，函数内部expon参数即是利用目前结果与实际结果是否相同决定e指数系数的正负的。当然，为保证D总和为1，所以需要各自对应的值再除以当前总和。
那么为什么在第二轮分类的时候不能只考虑第一轮分错的三个呢？因为每一个分类器最后的叠加使用是是不能专门表明我需要应用在哪几个数据上的，所以依照第二个的分类器，会有一部分第一次分类正确的结果现在分类错误，或者说，单独使用分类器二肯定不如单独使用分类器一。（废话，分类器一是单独使用的最优解。）据此，已经解释清楚为什么需要调整D值以及也要对第一次分类正确的数组设立不为0的D值。
那么现在问题来了，如果分类器一和二共同使用，那么它们的影响一样吗？要五五分吗？可以预想，如果数组3在分类器一的作用下分类正确，而在分类器二的作用下分类错误，如果一和二同等重要，那么数组3的分类结果就成了既对又错。因此，我们需要对每个分类器加上系数。以第一个分类器为例，这个系数与通过它针对数据判断得到的误差有关，相应的第二个的系数与第二个分类器误差有关。
如果这样来说，第一个第二个或者第n个各行其是，这样的总和岂不是乱套了？
因此分类器的停止判据并不是第多少个能判断对，（事实上第一轮没成功后续也不可能单独成功），而是得到了第二个分类器，那就把一和二综合使用，看是否能够完全判断正确，不能则根据第二轮的D值，作为第三轮的初始值。可以看到，每一个分类器的效用都受前面所有分类器总和的影响，它的目的是在前n个基础上去得到新的分类器，再综合使用。但是否每一次新的分类器的叠加都一定更好呢，答案是否定的，就我跑了书中实际算例来说，我将分类器数目叠加到50个，输出其中每次新得到分类器后的总误差，总的来说是在降低，但也是有波动和起伏的，甚至50个的效果和10的差别不大。这在一定程度上也说明了它不够完美。
不够完美的原因自然是权重和分类器系数的原因。或许存在更好的权重分配方法或者分类器系数生成方法，毕竟现有的方法只是从数学层面上论证可以实现，不代表它们最完美。我目前推测，有时候多了一个分类器不能得到更好的优化的原因在于，每一次的D值与当下的分类器密切相关，但是作为新一轮的分类器，它想改良的是前面所有分类器的总结果，但它接受的D值受前一轮影响很大，再往前的分类器对它影响甚微。我猜测，如果能够建立在之前所有分类器的预测结果的基础上计算D，可能会加速改良结果。

说远了，最后还有adaClassify(datToClass,classifierArr)函数。第一项可以输出随便的新数据，特征值与之前训练的特征值项数一致，第二项为训练得到的多个简单分类器，在这个函数中将分类器共同作用，最后以正负判定为1/-1，返回预测结果。事实上如果我们再次把训练集的数据（loadSimpData() 函数中的数据组）放上来，将得到的三个分类器用进去，确实发现所有的预测都与实际分类结果相同，这也说明了分离器在处理简单数据组时的有效性。

最后总结：
选择某个单一特征项，再选某一个数值，就可以将一系列数据组分类两类，这样的结果与实际可能有偏差。
于是通过同样的方法，再得到新的分类器，综合使用，看是否还有偏差，有，在最大允许分类器范围内则继续。
每一次分类器的使用，都是要解决之前所有分类器综合使用分类错的数据组，因此需要加重分错数据组的权重，降低分对数值的权重。
多个分类器使用，每个对预测结果的影响程度不同，影响系数与各自分类器使用时分类结果的误差有关。而误差的计算受D影响，即间接受前面结果的影响，因此增强了耦合度，改良效果较好。
